{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful settings for interactive work\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import basic_functions as bf\n",
    "import extended_functions as ef\n",
    "import training_functions as tf\n",
    "import torch\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NN variables\n",
    "hidden_sizes_v1 = [1000, 512, 256, 128, 7]\n",
    "hidden_sizes_v2 = [1000, 256, 64, 7]\n",
    "hidden_sizes_v3 = [1000, 128, 7]\n",
    "hidden_sizes_v4 = [1000, 256, 256, 7]\n",
    "hidden_sizes = [hidden_sizes_v1, hidden_sizes_v2, hidden_sizes_v3, hidden_sizes_v4]\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "Bmods  = [BasicMLP(hidden_size) for hidden_size in hidden_sizes]\n",
    "E1mods = [ExtendedMLP_v1(hidden_size) for hidden_size in hidden_sizes]\n",
    "E2mods = [ExtendedMLP_v2(hidden_size) for hidden_size in hidden_sizes]\n",
    "Smods  = [SuperExtendedMLP(hidden_sizes_v1)]\n",
    "\n",
    "Bopts = [torch.optim.Adam(model.parameters()) for model in Bmods]\n",
    "E1opts = [torch.optim.Adam(model.parameters()) for model in E1mods]\n",
    "E2opts = [torch.optim.Adam(model.parameters()) for model in E2mods]\n",
    "Sopts = [torch.optim.Adam(model.parameters()) for model in Smods]\n",
    "\n",
    "Neps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Training: basic001\n",
      "Epoch:   0 | Loss: 1.71\n",
      "Epoch:  50 | Loss: 0.48\n",
      "Epoch: 100 | Loss: 0.47\n",
      "Epoch: 150 | Loss: 0.48\n",
      "Epoch: 200 | Loss: 0.48\n",
      "Epoch: 250 | Loss: 0.48\n",
      "Epoch: 300 | Loss: 0.48\n",
      "Epoch: 350 | Loss: 0.47\n",
      "Epoch: 400 | Loss: 0.49\n",
      "Epoch: 450 | Loss: 0.56\n",
      "----------------------------------------------------\n",
      "Test Loss: 0.6793\n",
      "====================================================\n",
      "Training: basic002\n",
      "Epoch:   0 | Loss: 1.65\n",
      "Epoch:  50 | Loss: 0.56\n",
      "Epoch: 100 | Loss: 0.56\n",
      "Epoch: 150 | Loss: 0.44\n",
      "Epoch: 200 | Loss: 0.43\n",
      "Epoch: 250 | Loss: 0.42\n",
      "Epoch: 300 | Loss: 0.42\n",
      "Epoch: 350 | Loss: 0.42\n",
      "Epoch: 400 | Loss: 0.43\n",
      "Epoch: 450 | Loss: 0.42\n",
      "----------------------------------------------------\n",
      "Test Loss: 0.6466\n",
      "====================================================\n",
      "Training: basic003\n",
      "Epoch:   0 | Loss: 1.56\n",
      "Epoch:  50 | Loss: 0.56\n",
      "Epoch: 100 | Loss: 0.56\n",
      "Epoch: 150 | Loss: 0.56\n",
      "Epoch: 200 | Loss: 0.61\n",
      "Epoch: 250 | Loss: 0.42\n",
      "Epoch: 300 | Loss: 0.42\n",
      "Epoch: 350 | Loss: 0.42\n",
      "Epoch: 400 | Loss: 0.43\n",
      "Epoch: 450 | Loss: 0.42\n",
      "----------------------------------------------------\n",
      "Test Loss: 0.6523\n",
      "====================================================\n",
      "Training: basic004\n",
      "Epoch:   0 | Loss: 1.50\n",
      "Epoch:  50 | Loss: 0.43\n",
      "Epoch: 100 | Loss: 0.43\n",
      "Epoch: 150 | Loss: 0.46\n",
      "Epoch: 200 | Loss: 0.43\n",
      "Epoch: 250 | Loss: 0.43\n",
      "Epoch: 300 | Loss: 0.41\n",
      "Epoch: 350 | Loss: 0.51\n",
      "Epoch: 400 | Loss: 0.42\n",
      "Epoch: 450 | Loss: 0.42\n",
      "----------------------------------------------------\n",
      "Test Loss: 0.6442\n"
     ]
    }
   ],
   "source": [
    "# Train the basic models\n",
    "tn_lder, tt_lder = bf.get_data(0.8)\n",
    "\n",
    "for idx,(model, optimizer) in enumerate(zip(Bmods, Bopts)):\n",
    "    model_name = \"basic\"+str(idx+1).zfill(3)\n",
    "    print(\"====================================================\")\n",
    "    print(\"Training: \"+model_name)\n",
    "    tf.train_model(model, criterion, optimizer, tn_lder, model_name, Neps)\n",
    "    print(\"----------------------------------------------------\")\n",
    "    tf.test_model(model, criterion, tt_lder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Training: extd001_v1\n",
      "Epoch:   0 | Loss: 1.35\n",
      "Epoch:  50 | Loss: 0.46\n",
      "Epoch: 100 | Loss: 0.44\n",
      "Epoch: 150 | Loss: 0.42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m====================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_name)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtn_lder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNeps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m tf\u001b[38;5;241m.\u001b[39mtest_model(model, criterion, tt_lder)\n",
      "File \u001b[0;32m~/StanfordMSL/Image2Pose/training_functions.py:24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, train_loader, model_name, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Optimize\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/i2p-env/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/i2p-env/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the extended models\n",
    "tn_lder, tt_lder = ef.get_data(0.8)\n",
    "Neps = 200\n",
    "\n",
    "for idx,(model, optimizer) in enumerate(zip(E1mods, E1opts)):\n",
    "    model_name = \"extd\"+str(idx+1).zfill(3)+\"_v1\"\n",
    "    print(\"====================================================\")\n",
    "    print(\"Training: \"+model_name)\n",
    "    tf.train_model(model, criterion, optimizer, tn_lder, model_name, Neps)\n",
    "    print(\"----------------------------------------------------\")\n",
    "    tf.test_model(model, criterion, tt_lder)\n",
    "\n",
    "for idx,(model, optimizer) in enumerate(zip(E2mods, E2opts)):\n",
    "    model_name = \"extd\"+str(idx+1).zfill(3)+\"_v2\"\n",
    "    print(\"====================================================\")\n",
    "    print(\"Training: \"+model_name)\n",
    "    tf.train_model(model, criterion, optimizer, tn_lder, model_name, Neps)\n",
    "    print(\"----------------------------------------------------\")\n",
    "    tf.test_model(model, criterion, tt_lder)\n",
    "\n",
    "for idx,(model, optimizer) in enumerate(zip(Smods, Sopts)):\n",
    "    model_name = \"spex\"+str(idx+1).zfill(3)\n",
    "    print(\"====================================================\")\n",
    "    print(\"Training: \"+model_name)\n",
    "    tf.train_model(model, criterion, optimizer, tn_lder, model_name, Neps)\n",
    "    print(\"----------------------------------------------------\")\n",
    "    tf.test_model(model, criterion, tt_lder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2p-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
