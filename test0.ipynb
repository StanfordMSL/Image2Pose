{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing of ResNet+MLP where we take the ResNet output, concatenate it with a gravity vector (which the drone has via its IMU) and pass it through an MLP to get an estimate of the pose.\n",
    "\n",
    "We try out a bunch of hidden layer configurations and also compare results between networks with pre-trained ResNet weights and networks where the ResNet weights are allowed to be trained on given the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful settings for interactive work\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lowjunen/anaconda3/envs/nerfstudio/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import data_functions as df\n",
    "import training_functions as tf\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NN variables\n",
    "hidden_sizes = [\n",
    "    [256, 128, 7],\n",
    "    [128, 32, 7],\n",
    "    [128, 32, 8, 7]\n",
    "    ]\n",
    "\n",
    "mlp0 = [VisionPoseMLP(hidden_size,True) for hidden_size in hidden_sizes]\n",
    "mlp1 = [VisionPoseMLP(hidden_size,False) for hidden_size in hidden_sizes]\n",
    "mlps:List[VisionPoseMLP] = mlp0 + mlp1\n",
    "\n",
    "Neps = 500\n",
    "Ndata = None\n",
    "ratio = 0.8\n",
    "Nexp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/transforms.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate data loaders\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/StanfordMSL/Image2Pose/data_functions.py:39\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(ratio, Ndata)\u001b[0m\n\u001b[1;32m     35\u001b[0m nerf_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/images\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# nerf_file = 'data/mocap'\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Load frames\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransforms_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m     40\u001b[0m     frames \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Initialize the inference transforms\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/transforms.json'"
     ]
    }
   ],
   "source": [
    "# Generate data loaders\n",
    "train_loader, test_loader = df.get_data(ratio,Ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "Training: basic001\n",
      "Epoch:  50 | Loss: 0.07464\n",
      "Epoch: 100 | Loss: 0.01916\n",
      "Epoch: 150 | Loss: 0.04128\n",
      "Epoch: 200 | Loss: 0.01285\n",
      "Epoch: 250 | Loss: 0.01037\n",
      "Epoch: 300 | Loss: 0.00661\n",
      "Epoch: 350 | Loss: 0.01018\n",
      "Epoch: 400 | Loss: 0.00810\n",
      "Epoch: 450 | Loss: 0.00959\n",
      "Epoch: 500 | Loss: 0.00432\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "Test Loss: 0.0750\n",
      "Examples:\n",
      "=============================================================\n",
      "Training: basic002\n",
      "Epoch:  50 | Loss: 0.16328\n",
      "Epoch: 100 | Loss: 0.09442\n",
      "Epoch: 150 | Loss: 0.09258\n",
      "Epoch: 200 | Loss: 0.04151\n",
      "Epoch: 250 | Loss: 0.03373\n",
      "Epoch: 300 | Loss: 0.02805\n",
      "Epoch: 350 | Loss: 0.02112\n",
      "Epoch: 400 | Loss: 0.01124\n",
      "Epoch: 450 | Loss: 0.02359\n",
      "Epoch: 500 | Loss: 0.01014\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "Test Loss: 0.1085\n",
      "Examples:\n",
      "=============================================================\n",
      "Training: basic003\n",
      "Epoch:  50 | Loss: 0.27491\n",
      "Epoch: 100 | Loss: 0.17466\n",
      "Epoch: 150 | Loss: 0.13434\n",
      "Epoch: 200 | Loss: 0.08443\n",
      "Epoch: 250 | Loss: 0.05338\n",
      "Epoch: 300 | Loss: 0.05577\n",
      "Epoch: 350 | Loss: 0.03655\n",
      "Epoch: 400 | Loss: 0.03244\n",
      "Epoch: 450 | Loss: 0.02356\n",
      "Epoch: 500 | Loss: 0.01909\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "Test Loss: 0.1329\n",
      "Examples:\n",
      "=============================================================\n",
      "Training: basic004\n",
      "Epoch:  50 | Loss: 0.01731\n",
      "Epoch: 100 | Loss: 0.00657\n",
      "Epoch: 150 | Loss: 0.00558\n",
      "Epoch: 200 | Loss: 0.00443\n",
      "Epoch: 250 | Loss: 0.03907\n",
      "Epoch: 300 | Loss: 0.00489\n",
      "Epoch: 350 | Loss: 0.00500\n",
      "Epoch: 400 | Loss: 0.00592\n",
      "Epoch: 450 | Loss: 0.00201\n",
      "Epoch: 500 | Loss: 0.00329\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "Test Loss: 0.0189\n",
      "Examples:\n",
      "=============================================================\n",
      "Training: basic005\n",
      "Epoch:  50 | Loss: 0.01560\n",
      "Epoch: 100 | Loss: 0.00567\n",
      "Epoch: 150 | Loss: 0.00617\n",
      "Epoch: 200 | Loss: 0.00512\n",
      "Epoch: 250 | Loss: 0.00310\n",
      "Epoch: 300 | Loss: 0.00222\n",
      "Epoch: 350 | Loss: 0.00256\n",
      "Epoch: 400 | Loss: 0.00265\n",
      "Epoch: 450 | Loss: 0.00186\n",
      "Epoch: 500 | Loss: 0.00190\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "Test Loss: 0.0122\n",
      "Examples:\n",
      "=============================================================\n",
      "Training: basic006\n",
      "Epoch:  50 | Loss: 0.04673\n",
      "Epoch: 100 | Loss: 0.02603\n",
      "Epoch: 150 | Loss: 0.01269\n",
      "Epoch: 200 | Loss: 0.00756\n",
      "Epoch: 250 | Loss: 0.00682\n",
      "Epoch: 300 | Loss: 0.00631\n",
      "Epoch: 350 | Loss: 0.00559\n",
      "Epoch: 400 | Loss: 0.00291\n",
      "Epoch: 450 | Loss: 0.00370\n",
      "Epoch: 500 | Loss: 0.01904\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "Test Loss: 0.0437\n",
      "Examples:\n"
     ]
    }
   ],
   "source": [
    "# Train the basic models\n",
    "for idx,mlp in enumerate(mlps):\n",
    "    mlp_name = \"basic\"+str(idx+1).zfill(3)\n",
    "    print(\"=============================================================\")\n",
    "    print(\"Training: \"+mlp_name)\n",
    "    tf.train_model(mlp, train_loader, mlp_name,useNeRF=False, Neps=Neps)\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    tf.test_model(mlp, test_loader,useNeRF=False, Nexp=Nexp)\n",
    "\n",
    "# print(mlp.networks[0].model[0].conv1.weight[0,0,0,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2p-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
